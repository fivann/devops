# 1. Pod Shared Volume: emptyDir: {}

# https://youtu.be/AXi2oENUJHo

# On-disk files in a Container are ephemeral, which presents some problems for non-trivial applications when running in Containers. 1. When a Container crashes, kubelet will restart it, but the files will be lost - the Container starts with a clean state. 2. When running Containers together in a Pod it is often necessary to share files between those Containers. The Kubernetes Volume abstraction solves both of these problems.

# An emptyDir volume is first created when a Pod is assigned to a Node, and exists as long as that Pod is running on that node. As the name says, it is initially empty. Containers in the Pod can all read and write the same files in the emptyDir volume, though that volume can be mounted at the same or different paths in each Container. When a Pod is removed from a node for any reason, the data in the emptyDir is deleted forever.

# Example Pod:
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  initContainers:
  - image: image1
    name: init-contanier-1
    volumeMounts:
    - mountPath: /dir1
      name: pod-shared-volume

  containers:
  - image: image2
    name: container-a
    volumeMounts:
    - mountPath: /dir2
      name: pod-shared-volume

  - image: image3
    name: container-b
    volumeMounts:
    - mountPath: /dir3
      name: pod-shared-volume

  volumes:
  - name: pod-shared-volume
    emptyDir: {}
    
# In this example, all containers (init-contanier-1, container-a, container-b) have access to the same data located inside their pod.

# Task
# We are going to run a pod with Tomcat application, will deploy root’s “server page” before main process starts.
# Please, create a Pod with Init and Regular Containers. Share filesystem between containers. Expose pod to NodePort.

# Requirements
# Pod:
# Name: tomcat-with-init-emptydir

# Init Container:
# Image: busybox:1.34
# command: wget -O /webapps/ROOT/index.jsp https://playpit-labs-assets.s3-eu-west-1.amazonaws.com/tomcat/index.jsp
# Volume Mount Path: /webapps/ROOT/

# Container:
# Image: tomcat:9.0-jre8-alpine
# Volume Mount Path: /usr/local/tomcat/webapps/ROOT
# Volume:
# Name: shared-pod-volume
# type: emptyDir

# Service:
# Name: tomcat-with-init-emptydir-svc
# Type: NodePort
# Target Port: 8080
# Node Port: 30080

apiVersion: v1
kind: Pod
metadata:
  name: tomcat-with-init-emptydir
  ## LABELS IS EVERITHING!
  labels:
    app: tomcat-with-init-emptydir
spec:
  initContainers:
  - name: init-container
    image: busybox:1.34
    command: ["wget", "-O", "/webapps/ROOT/index.jsp", "https://playpit-labs-assets.s3-eu-west-1.amazonaws.com/tomcat/index.jsp"]
    volumeMounts:
    - name: shared-pod-volume
      mountPath: /webapps/ROOT/
  containers:
  - name: tomcat-container
    image: tomcat:9.0-jre8-alpine
    volumeMounts:
    - name: shared-pod-volume
      mountPath: /usr/local/tomcat/webapps/ROOT
  volumes:
  - name: shared-pod-volume
    emptyDir: {}
---
# In this example, the pod tomcat-with-init-emptydir contains two containers, an "init container" and a "regular container". The init container init-container is used to download a file from the internet, it uses the wget command to download index.jsp and save it in /webapps/ROOT/ directory. Then the init container mounts a volume shared-pod-volume in the path /webapps/ROOT/.

# The regular container tomcat-container is running the Tomcat application and it also mounts the volume shared-pod-volume in the path /usr/local/tomcat/webapps/ROOT, which means that the regular container can access the same files that init container has created in the shared volume.

# This pod also uses an emptyDir type of volume named shared-pod-volume, this type of volume is created when a Pod is assigned to a node, and exists as long as that Pod is running on that node. An emptyDir volume is initially empty when a Pod is created.

# To expose this pod to a NodePort you need to create a Service:
---
apiVersion: v1
kind: Service
metadata:
  name: tomcat-with-init-emptydir-svc
spec:
  selector:
    app: tomcat-with-init-emptydir
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    nodePort: 30080
  type: NodePort


# Documentation
# https://kubernetes.io/docs/concepts/storage/volumes/#emptydir

# Pod Shared Volume: emptyDir: {}

# Here’s a little bit tricky task.

# Please run a pod with custom details. Using initContainer you should compose an index.html with custom data.

# Requirements:
# Pod:
# Name: web-tricky-pod-emptydir
# Shared Volume:
# Type: emptyDir
# Name: shared-pod-volume

# Init Container (initiates index.html file at Pod starts):
# Image: busybox:1.34
# Command: simple shell script which creates an index.html page, for example:
# cat << END > /html/index.html
# <html>
#   <head></head>
#   <body>
#     <b>Student</b>: Ivan Fedorov2<br>
#     <b>pod name</b>: $(hostname)<br>
#     <b>pod ip</b>: $(hostname -i)<br>
#   </body>
# </html>
# END

# Container (shows index.html with nginx web service):
# Image: nginx:alpine
# mountPath: /usr/share/nginx/html

# Service:
# Name: web-tricky-pod-emptydir-svc
# Type: NodePort
# NodePort: 30081

# General strategy:
# Mount emptyDir volume into initContainer and create index.html file
# Figure out where nginx is looking for index.html to respond to a web request
# Mount that emptyDir volume (1) into nginx container (2)

apiVersion: v1
kind: Pod
metadata:
  name: web-tricky-pod-emptydir
  labels:
    app: web-tricky-pod-emptydir
spec:
  volumes:
  - name: shared-pod-volume
    emptyDir: {}
  initContainers:
  - name: init-html
    image: busybox:1.34
    command: ["sh", "-c", "cat << END > /html/index.html\n<html>\n  <head></head>\n  <body>\n    <b>Student</b>: Ivan Fedorov2<br>\n    <b>pod name</b>: $(hostname)<br>\n    <b>pod ip</b>: $(hostname -i)<br>\n  </body>\n</html>\nEND"]
    volumeMounts:
    - name: shared-pod-volume
      mountPath: /html
  containers:
  - name: web
    image: nginx:alpine
    volumeMounts:
    - name: shared-pod-volume
      mountPath: /usr/share/nginx/html
---
apiVersion: v1
kind: Service
metadata:
  name: web-tricky-pod-emptydir-svc
spec:
  selector:
    app: web-tricky-pod-emptydir
  type: NodePort
  ports:
  - name: http
    port: 80
    nodePort: 30081

# This file creates a pod with the specified name, "web-tricky-pod-emptydir". The pod has a single volume of type "emptyDir" with the specified name "shared-pod-volume". The pod has two containers, the first one is initContainers that runs the "busybox:1.34" image, the command creates the index.html file in the /html folder of the shared volume, and the "init-html" container will use the shared volume to create the index.html file
# The second container runs the "nginx:alpine" image and is used to serve the index.html file from the shared volume. The container's "web" volumeMounts points to the shared volume and the path of /usr/share/nginx/html, nginx looks for the index.html here.
# Than it creates a service with the name "web-tricky-pod-emptydir-svc" that directs traffic to the pods that are selected by the app=web-tricky-pod-emptydir, and it opens the port 30081 on the nodes to be able to access the service.


# Pod Shared Volume: Memory

# By default, emptyDir volumes are stored on whatever medium is backing the node - that might be disk or SSD or network storage, depending on your environment. However, you can set the emptyDir.medium field to Memory to tell Kubernetes to mount a tmpfs (RAM-backed filesystem) for you instead. While tmpfs is very fast, be aware that unlike disks, tmpfs is cleared on a node reboot, and any files you write will count against your Container’s memory limit.

#   ...
#   volumes:
#   - name: pod-shared-volume-ram
#     emptyDir:
#       medium: Memory
      
# Task
# Create a Pod with Init and Regular Containers. Share in-pod filesystem bettween them.
# Requirements
# Pod:
# Pod Name: web-pod-emptydir-tmpfs
# Shared Storage:
# Type: emptyDir
# Medium: Memory

# Init Container:
# Image: busybox:1.34
# Command: echo "initialized by initContainer, using tmpfs" > /html/index.html
# Volume Mount Path: /html

# Container:
# Image: nginx:alpine
# Volume Mount Path: /usr/share/nginx/html
# Type: emptyDir
# Medium: Memory
# Volume Name: shared-pod-volume

# Service:
# Name: web-pod-emptydir-tmpfs-svc
# Type: NodePort
# Target Port: 80
# Node Port: 30082

apiVersion: v1
kind: Pod
metadata:
  name: web-pod-emptydir-tmpfs
  labels:
    app: web-pod-emptydir-tmpfs
spec:
  volumes:
  - name: shared-pod-volume
    emptyDir:
      medium: Memory
  initContainers:
  - name: init-html
    image: busybox:1.34
    command: ["sh", "-c", "echo 'initialized by initContainer, using tmpfs' > /html/index.html"]
    volumeMounts:
    - name: shared-pod-volume
      mountPath: /html
  containers:
  - name: web
    image: nginx:alpine
    volumeMounts:
    - name: shared-pod-volume
      mountPath: /usr/share/nginx/html
---
apiVersion: v1
kind: Service
metadata:
  name: web-pod-emptydir-tmpfs-svc
spec:
  selector:
    app: web-pod-emptydir-tmpfs
  type: NodePort
  ports:
  - name: http
    port: 80
    nodePort: 30082

## Running Docker inside Kubernetes’ Pod
# We will try to run docker container inside Kubernetes

# Task
# Let’s try to create a pod with docker:dind image. On Linux Server we’d run this command:

# docker run --privileged -d \
#     --tmpfs /var/lib/docker \
#     docker:dind
# Here, we use tmpfs just for speeding up Docker engine

# But what about translating this bare docker run command into the Pod?

# Verify:
# Check if docker pod is running

# $ kubectl get pods -o custom-columns="POD:.metadata.name,IMAGE:.spec.containers[*].image,STATUS:.status.phase"
# POD      IMAGE         STATUS
# docker   docker:dind   Running
# Check if all good with docker daemon

# $ kubectl exec -it docker -- sh
# / # docker ps
# CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES
# / # docker run hello-world

############################################################################################################################################

# Mounting Data from Node: hostPath

# A hostPath volume mounts a file or directory from the host node’s filesystem into your Pod. This is not something that most Pods will need, but it offers a powerful escape hatch for some applications.

# For example, some uses for a hostPath are:
#   running a Container that needs access to Docker internals; use a hostPath of /var/lib/docker
#   running cAdvisor in a Container; use a hostPath of /sys
#   allowing a Pod to specify whether a given hostPath should exist prior to the Pod running, whether it should be created, and what it should exist as

# Example Pod:

# apiVersion: v1
# kind: Pod
# metadata:
#   name: test-pd
# spec:
#   containers:
#   - image: k8s.gcr.io/test-webserver
#     name: test-container
#     volumeMounts:
#     - mountPath: /test-pd
#       name: test-volume
#   volumes:
#   - name: test-volume
#     hostPath:
#       path: /data              # directory location on host
#       type: DirectoryOrCreate  # this field is optional
      
Defining a Node where to run a workload
You can constrain a Pod to only be able to run on particular Node(s), or to prefer to run on particular nodes.
There are several ways to do this:

# 1. Node Name
# Please find more details here:
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodename

# apiVersion: v1
# kind: Pod
# ...
# spec:
#   nodeName: node01
#   containers: ...
  
# 2. Node Selector
# Please find more details here:
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#built-in-node-labels

# apiVersion: v1
# kind: Pod
# ...
# spec:
#   nodeSelector:
#     kubernetes.io/hostname: node01
#   containers: ...
  
# 3. Node Affinity
# Please find more details here:

# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity
# apiVersion: v1
# kind: Pod
# ...
# spec:
#   affinity:
#     nodeAffinity:
#       requiredDuringSchedulingIgnoredDuringExecution:
#         nodeSelectorTerms:
#         - matchExpressions:
#           - key: kubernetes.io/hostname
#             operator: In
#             values:
#             - node01
#   containers: ...
  
# Task:
# We have a static site. Its content placed under /host-data folder on node01. Please create a microservice application with volume mount from host location. You will see working site inside a browser here.

# Requirements:
# Deployment:
# Name: web-deployment-hostpath
#  Set pod to run on node01!
# Image: nginx:alpine
# Replicas: 1
# Volume:
# hostPath: /host-data (on node01)
# mount point: /usr/share/nginx/html

# Service:
# Name: web-deployment-hostpath-svc
# Type: NodePort
# NodePort: 30083


apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment-hostpath
  labels:
    app: web-deployment-hostpath
spec:
  replicas: 1
  selector:
    matchLabels:
      app: web-deployment-hostpath
  template:
    metadata:
      labels:
        app: web-deployment-hostpath
    spec:
      nodeSelector:
        kubernetes.io/hostname: node01
      containers:
      - name: web
        image: nginx:alpine
        volumeMounts:
        - name: web-data
          mountPath: /usr/share/nginx/html
      volumes:
      - name: web-data
        hostPath:
          path: /host-data
---
apiVersion: v1
kind: Service
metadata:
  name: web-deployment-hostpath-svc
spec:
  selector:
    app: web-deployment-hostpath
  type: NodePort
  ports:
  - name: http
    port: 80
    nodePort: 30083




# Documentation:
# https://kubernetes.io/docs/concepts/storage/volumes/#hostpath
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodename
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#built-in-node-labels


############################################################################################################################################ 



# Persistent Volumes

# The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.

# A PersistentVolume (PV):
# is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.

# A PersistentVolumeClaim (PVC):
# It’s a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted once read/write or many times read-only).

# While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than just size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the StorageClass resource.

# PV & PVC Explained:

# https://youtu.be/hAhoeg3RryY
# https://youtu.be/x2sMWUkasoE

# $ kubectl get pv
# $ kubectl get pv <<pv name>> -o yaml
# $ kubectl describe pv <<pv name>>

# Documentation:
# https://kubernetes.io/docs/concepts/storage/persistent-volumes/
# https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming
# https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes
# https://www.learnitguide.net/2020/03/kubernetes-persistent-volumes-and-claims.html

# When a consumer is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted

# The Retain reclaim policy allows for manual reclamation of the resource
# The Delete reclaim policy removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure (such as an AWS EBS or so on)
# The Recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim -  Only if supported by the underlying volume plugin

# There are two ways PVs may be provisioned: statically or dynamically:

# Static way means that you have to specify PV and PVC should be configured to utilize existing PVs - all what we did on previous scenarios
# Dynamic implies the storage resources are dynamically provisioned using the provisioner specified by the StorageClass object (see user-guide). StorageClasses are essentially blueprints that abstract away the underlying storage provider, as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).
# First, let’s check what StorageClasses our Cluster has

# kubectl get storageclass << sc_name >>
# kubectl describe storageclass << sc_name >>

##########################################################################################
## PVC PV Troubleshooting 
# $ kubectl edit pv pv-trouble
# $ kubectl edit pvc pv-trouble-claim 
# Check if AccessModel and Claiming size are mathcing!

##########################################################################################

# Task
# There is  mysql-data pv. Please create deployment object and respective persistent volume claim based on following requirements:

# Requirements:
# Deployment Name: mysql-db
# Image Name: mysql:8
# Replicas: 1
# Environment Variables:
# MYSQL_RANDOM_ROOT_PASSWORD: true
# MYSQL_DATABASE: production
# MYSQL_USER: ifedorov2
# MYSQL_PASSWORD: pAssw0rd
# Container Port: 3306
# Store MySQL Data in Kubernetes PV:
# PV Name: mysql-data
# PVC Name: mysql-data-claim
# mountPath: /var/lib/mysql

# Please find a draft of the application stack in ~/mysql-app.yaml

#  Wait till deployment finishes

# Note:
# For DB and other stateful applications, the best choice is to use StatefulSets!

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-db
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql-db
  template:
    metadata:
      labels:
        app: mysql-db
    spec:
      containers:
      - name: mysql-db
        image: mysql:8
        env:
        - name: MYSQL_RANDOM_ROOT_PASSWORD
          value: "true"
        - name: MYSQL_DATABASE
          value: production
        - name: MYSQL_USER
          value: ifedorov2
        - name: MYSQL_PASSWORD
          value: pAssw0rd
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-data
        persistentVolumeClaim:
          claimName: mysql-data-claim
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-data-claim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
  selector:
    matchLabels:
      pv: mysql-data
##########################################################################################

# RECLAIMING 

# When a consumer is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted

# The Retain reclaim policy allows for manual reclamation of the resource
# The Delete reclaim policy removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure (such as an AWS EBS or so on)
# The Recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim -  Only if supported by the underlying volume plugin
# Documentation:
# https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming

##########################################################################################

# Storage Classes

# There are two ways PVs may be provisioned: statically or dynamically:
#   Static way means that you have to specify PV and PVC should be configured to utilize existing PVs - all what we did on previous scenarios
#   Dynamic implies the storage resources are dynamically provisioned using the provisioner specified by the StorageClass object (see user-guide). StorageClasses are essentially blueprints that abstract away the underlying storage provider, as well as other parameters, like disk-type (e.g.; solid-state vs standard disks).

# First, let’s check what StorageClasses our Cluster has
# $ kubectl get storageclass << sc_name >>
# $ kubectl describe storageclass << sc_name >>


# Creating PVC with StorageClass
# Here’s an example of PVC definition:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
  namespace: testns
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  # Storage Class to be used
  storageClassName: gold
  
# Task:
# Please create PVC using Storage Class Name playpit-labs-storage

# Requirements:
# PVC Name: pvc-with-sc
# PVC Requested Size: 2Gi
# PVC Namespace: default
# Labels:
# pvc=2gbi

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-with-sc
  labels:
    pvc: 2gbi
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: playpit-labs-storage

# This configuration creates a PVC named pvc-with-sc with a requested storage size of 2Gi and storage class playpit-labs-storage in the namespace default, it also sets the label "pvc: 2gbi" in the metadata section.

# When a pod requests the PVC, Kubernetes dynamically provisions the volume based on the storage class requested, the storage class has to be pre-configured in the cluster.
# Please make sure that the storage class 'playpit-labs-storage' exist in the cluster, also the namespace 'default' should exist before you apply the yaml file.

##########################################################################################


# Using PVC as Volume

# Let’s check our pvc right after its creation:

# $ kubectl get pvc pvc-with-sc
# NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS           AGE
# pvc-with-sc   Pending                                      playpit-labs-storage   2m21s
# It’s in status Pending which means that PV will be created once the first its consumer appears. Need to create a Pod using this pvc.

# Let’s do this

# Task:
# Create a Pod which uses pvc-with-sc pvc

# Requirements:
# Pod name: testing-pvc-sc
# Container image: busybox:1.34
# Container command: sleep infinity
# Volume:
# persistentVolumeClaim: pvc-with-sc
# volume Mount: /data


apiVersion: v1
kind: Pod
metadata:
  name: testing-pvc-sc
spec:
  containers:
  - name: busybox
    image: busybox:1.34
    command: ["sleep", "infinity"]
    volumeMounts:
    - name: pvc-data
      mountPath: /data
  volumes:
  - name: pvc-data
    persistentVolumeClaim:
      claimName: pvc-with-sc


# Verify:
# $ kubectl get pv
# NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                             STORAGECLASS           REASON   AGE
# ...
# pvc-2d4be53d-d151-459d-8a0b-9f1315a19142   2Gi        RWO            Delete           Bound    default/pvc-with-sc               playpit-labs-storage            2m4s
# ...

# $ kubectl get pvc pvc-with-sc
# NAME                      STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           AGE
# pvc-with-sc               Bound    pvc-2d4be53d-d151-459d-8a0b-9f1315a19142   2Gi        RWO            playpit-labs-storage   2m59s

# $ kubectl get po testing-pvc-sc
# NAME             READY   STATUS    RESTARTS   AGE
# testing-pvc-sc   1/1     Running   0          38s

# In this configuration, you are creating a Pod named testing-pvc-sc that runs a single container based on the image busybox:1.34 and this container runs the command sleep infinity.
# The Pod also mounts a volume named pvc-with-sc to the path /data inside the container, this volume is using the previously created PVC pvc-with-sc.
# It's important to make sure that the pvc pvc-with-sc exist and accessible in the cluster before you apply the yaml file.

##########################################################################################

# Setting “default” StorageClass

# Kubernetes v1.6 added the ability to set a default storage class. This is the storage class that will be used to provision a PV if a user does not specify any one in a PVC.
# You can define a default storage class by setting the annotation storageclass.kubernetes.io/is-default-class to true in the storage class definition. According to the specification, any other value or absence of the annotation is interpreted as false.

# It is possible to configure an existing storage class to be the default storage class by using the following command:

kubectl patch storageclass <storage-class-name> -p '{
  "metadata": {
    "annotations": {
      "storageclass.kubernetes.io/is-default-class":"true"
    }
  }
}'
# Similarly, you can remove the default storage class annotation by using the following command:

kubectl patch storageclass <storage-class-name> -p '{
  "metadata": {
    "annotations": {
      "storageclass.kubernetes.io/is-default-class":"false"
    }
  }
}'

# Task:
# Set playpit-labs-storage a default StorageClass for our Cluster

kubectl patch storageclass playpit-labs-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# Verification:
# Before patching:
# $ kubectl get sc
# NAME                    PROVISIONER             ...
# playpit-labs-storage    rancher.io/local-path   ...

# After patching:
# $ kubectl get sc
# NAME                             PROVISIONER             ...
# playpit-labs-storage (default)   rancher.io/local-path   ...

# Documentation:
# https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/

##########################################################################################

# Using default StorageClass

# Usually all “turn-key” Kubernetes Clusters have “built-in” storage classes and their “native” provisioners.
# The following table provides more detail on default storage classes pre-installed by cloud provider as well as the specific parameters used by these defaults.

# Cloud Provider	      Default StorageClass Name	  Default Provisioner
# Amazon Web Services   gp2                       	aws-ebs
# Microsoft Azure	      standard	                  azure-disk
# Google Cloud Platform	standard	                  gce-pd
# OpenStack	            standard	                  cinder
# VMware vSphere	      thin	                      vsphere-volume

# How do I use a default StorageClass?
# If your cluster has a default StorageClass that meets your needs, then all you need to do is create a PersistentVolumeClaim (PVC) and the default provisioner will take care of the rest – there is no need to specify the storageClassName:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mypvc
  namespace: testns
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
      
# Task:
# Please create PVC using default StorageAccount (without mentioning playpit-labs-storage)

# Requirements:
# PVC Name: pvc-with-default-sc
# PVC Requested Size: 2Gi
# PVC Namespace: default

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-with-default-sc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi

# In this configuration, the PVC named pvc-with-default-sc is created in the namespace default, it requests for a storage size of 2Gi and it does not mention any storage class.
# When a pod requests the PVC and does not mention any storage class name, Kubernetes will automatically use the default storage class that was configured in the cluster.
# Please make sure that the namespace 'default' exists before you apply the yaml file.

##########################################################################################

# Creating Custom StorageClass
# To add your own storage class, first determine which provisioners work in your cluster. Then, create a StorageClass object with parameters customized to meet your needs. For many users, the easiest way to create the object is to write a yaml file and apply it with kubectl create -f. The following is an example of a StorageClass for Google Cloud Platform named gold that creates a pd-ssd. Since multiple classes can exist within a cluster, the administrator may leave the default enabled for most workloads (since it uses a pd-standard), with the gold class reserved for workloads that need extra performance.

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gold
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
  
# Task:
# Create own StorageClass named ifedorov2-storage-class. Find which provisioner is used by playpit-labs-storage and use it.
#  Obviously it should be similar to playpit-labs-storage as much as possible :) Don’t set any other parameters rather then required.

apiVersion: v1
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ifedorov2-storage-class
provisioner: <provisioner>

$ kubectl describe storageclass playpit-labs-storage | grep Provisioner

apiVersion: v1
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ifedorov2-storage-class
provisioner: rancher.io/local-path

In this configuration, you are creating a new Storage Class named ifedorov2-storage-class and set the provisioner to be the same as the playpit-labs-storage class.

# To find the provisioner name you can use the kubectl command:
#   $ kubectl describe storageclass playpit-labs-storage 
# this will give you a detailed view of the storage class including the provisioner name.

# You can also use this command to view all storage classes available

#   $ kubectl get storageclass 
# Please make sure that the provisioner is accessible and configured in your cluster, also the user running the command should have enough permissions to create a new storage class.

# Can I delete/turn off the default StorageClasses?
# You cannot delete the default storage class objects provided. Since they are installed as cluster addons, they will be recreated if they are deleted.
# You can, however, disable the defaulting behavior by removing (or setting to false) the following annotation: storageclass.beta.kubernetes.io/is-default-class.

# If there are no StorageClass objects marked with the default annotation, then PersistentVolumeClaim objects (without a StorageClass specified) will not trigger dynamic provisioning. They will, instead, fall back to the legacy behavior of binding to an available PersistentVolume object.

# Can I assign my existing PVs to a particular StorageClass?
# Yes, you can assign a StorageClass to an existing PV by editing the appropriate PV object and adding (or setting) the desired storageClassName field to it.
# What happens if I delete a PersistentVolumeClaim (PVC)?
# If the volume was dynamically provisioned, then the default reclaim policy is set to “delete”. This means that, by default, when the PVC is deleted, the underlying PV and storage asset will also be deleted. If you want to retain the data stored on the volume, then you must change the reclaim policy from “delete” to “retain” after the PV is provisioned.
# Reference:
# https://kubernetes.io/blog/2017/03/dynamic-provisioning-and-storage-classes-kubernetes/
