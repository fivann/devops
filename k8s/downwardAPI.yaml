# Injecting Pod Details into the Container as Environment Variables

# Following details can be exposed into container as environment variables:
#   metadata.name - Pod name (actually, container hostname is its pod name, by default)
#   metadata.namespace - Pod Namespace
#   spec.nodeName - Node name where pod/container is currently running
#   spec.serviceAccountName - Pod’s Service Account Name
#   status.podIP - Pod IP Address

# Here’s an example how to handle this:

apiVersion: v1
kind: Pod
metadata:
  name: pod-details-example
spec:
  containers:
    - name: main
      image: k8s.gcr.io/busybox
      command: 
      - sh
      - -c
      args:
      - |-
        printenv
        while true; do sleep 1; done
      env:
        - name: MY_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: MY_POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: MY_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: MY_POD_SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
              
# Checking how it behaves

# $ kubectl logs pod-details-example | grep MY_
# MY_POD_SERVICE_ACCOUNT=default
# MY_POD_NAMESPACE=default
# MY_POD_IP=10.244.1.20
# MY_NODE_NAME=node01
# MY_POD_NAME=pod-details-example

# Or this way:

# $ kubectl exec pod-details-example -- printenv MY_POD_NAMESPACE
# default
# $ kubectl exec pod-details-example -- printenv MY_NODE_NAME
# node01

# Good to know …
# Even without this ENV injection you can always find namespace where the pod is currently running in by inspecting /var/run/secrets/kubernetes.io/serviceaccount/namespace file
# How to know POD IP Address? Nothing stops you to use ifconfig, ip addr show or even hostname -I (works in Debian/Redhat containers) / hostname -i (works in Busybox, Alpine as well as in Debian/Redhat) commands inside the container.

# Task
# 1. Create a pod:
# namespace: default
# name: hamster
# image: busybox:1.34
# command: printenv; sleep infinity

# 2. Define following env variables from pod’s details:
# node Name, where pod is currently running on -> POD_NODE
# pod Namespace -> POD_NAMESPACE

# 3. Get pod’s logs into /tmp/default-hamster.log
# Save /tmp/default-hamster.log on client host

apiVersion: v1
kind: Pod
metadata:
  name: hamster
  namespace: default
spec:
  containers:
  - name: hamster
    image: busybox:1.34
    command: ["sh", "-c", "printenv; sleep infinity"]
    env:
    - name: POD_NODE
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace

# $ kubectl logs -f hamster -n default > /tmp/default-hamster.log


# Documentation
# https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/
# https://kubernetes.io/docs/reference/kubectl/cheatsheet/#interacting-with-running-pods
# https://kubernetes.io/docs/tasks/debug-application-cluster/debug-running-pod/#examine-pod-logs

#################################################################################################

# Pod’s Labels and Annotations

# Another mechanism for consuming the downward API is using a volume plug-in. The downward API volume plug-in creates a volume with configured fields projected into files. The metadata field of the VolumeSource API object is used to configure this volume. The plug-in supports the following fields:
#   Pod Name
#   Pod Namespace
#   Pod Annotations
#   Pod Labels

spec:
  containers:
    - volumeMounts:
        - name: podinfo
          mountPath: /podinfo
      ...
  volumes:
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
              
# Task:
# Change the deployment scapegoat to use podinfo volume. Pick all labels and annotations under /podinfo folder.

# Verify:
# POD=$(kubectl get pod -l application=scapegoat -o custom-columns="POD:.metadata.name" --no-headers)
# kubectl exec -it ${POD} -- cat /podinfo/annotations
# kubectl exec -it ${POD} -- cat /podinfo/labels

# Documentation:
# https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/
# Want to know more about Annotations and Labels?
# https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
# https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/

# Annotations Example:

spec:
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/agent-inject-status: "update"
        vault.hashicorp.com/agent-inject-secret-db-creds: "database/creds/db-app"
        vault.hashicorp.com/role: "db-app"
        vault.hashicorp.com/ca-cert: "/vault/tls/ca.crt"
        vault.hashicorp.com/client-cert: "/vault/tls/client.crt"
        vault.hashicorp.com/client-key: "/vault/tls/client.key"
        vault.hashicorp.com/tls-secret: "vault-tls-client"

##########################################################

# Pod’s Resources Fields

# When you specify a Pod, you can optionally specify how much of each resource a Container needs. The most common resources to specify are CPU and memory (RAM); there are others.

# When you specify the resource request for Containers in a Pod, the scheduler uses this information to decide which node to place the Pod on.

# When you specify a resource limit for a Container, the kubelet enforces those limits so that the running container is not allowed to use more of that resource than the limit you set. The kubelet also reserves at least the request amount of that system resource specifically for that container to use.

# This is crucial information when you plan cluster capacity to prevent under- or over-utilization.

Example:
apiVersion: v1
kind: Pod
metadata:
  name: dapi-envars-resourcefieldref
spec:
  containers:
    - name: container
      image: ...
      ...
      resources:
        requests:
          memory: "32Mi"
          cpu: "125m"
        limits:
          memory: "64Mi"
          cpu: "250m"
  ...
  
# More details:
# https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
# All of this is from the Infrastucture point of view, but what about applications? Do they know about these restrictions?

# You should keep in mind at least the following:
# If a Container exceeds its memory limit, it might be terminated.
# If a Container exceeds its memory request, it is likely that its Pod will be evicted whenever the node runs out of memory.
# How can we prevent this danger?

# Well, at least we can let application know about this outer (on container level) configuration by providing Environment variables or storing these details in some files.
# Let’s try.
# Use Container fields as values for environment variables
# Here’s an example:

apiVersion: v1
kind: Pod
metadata:
  name: dapi-envars-resourcefieldref
spec:
  containers:
    - name: test-container
      image: ...
      command: ...
      resources:
        requests:
          memory: "32Mi"
          cpu: "125m"
      env:
        - name: MY_CPU_REQUEST
          valueFrom:
            resourceFieldRef:
              containerName: test-container
              resource: requests.cpu
        - name: MY_CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: test-container
              resource: limits.cpu
              
# Task:
# Update scapegoat deployment to provide following environemnt variables into the main container:

# CPU_REQUEST: requests.cpu
# CPU_LIMIT: limits.cpu
# RAM_REQUEST: requests.memory
# RAM_LIMIT: limits.memory

# Verify:
# $ POD=$(kubectl get pod -l application=scapegoat -o custom-columns="POD:.metadata.name" --no-headers)
# $ kubectl exec -it ${POD} -- env | egrep '(REQUEST|LIMIT)'
# CPU_REQUEST=1
# CPU_LIMIT=1
# RAM_REQUEST=33554432
# RAM_LIMIT=67108864
