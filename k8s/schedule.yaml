## https://youtu.be/rDCWxkvPlAw

# 1. Scheduling Pod on a Specific Node

# Problem
# The primarily responsibility of kube-scheduler is to find a node where to place a pod. But how can we control this process?
# We’d like to run a pod on specific node by its name.

# Using nodeName Property
# It’s the simplest form of node selection constraint, but due to its limitations it is typically not used.

# Documentation:
# https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodename
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

# Example:

apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  nodeName: << node hostname >>
  containers:
  - name: main
    image: busybox:1.34
    command:
    - sleep
    - infinity
    
# Task:
# Please, update exercise deployment so that it runs pods on node01 host using nodeName only

# Verify:
# $ kubectl get pods -o wide
# NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
# exercise-6b4cb7c4cb-bp2hf   1/1     Running   0          61s   10.42.0.3   node01   <none>           <none>

####################################################################################################################################

# 2. Scheduling Pod on a Node by its Label
# Problem
# We’d like to run pods on any host from “a pool of suitable hosts”.

# Using nodeSelector Property
# The nodeSelector is the simplest recommended form of node selection constraint. nodeSelector is a field of PodSpec. It specifies a map of key-value pairs. For the pod to be eligible to run on a node, the node must have each of the indicated key-value pairs as labels (it can have additional labels as well). The most common usage is one key-value pair.

# Built-in node labels
# In addition to the labels you attach, nodes come pre-populated with a standard set of labels. These labels are:
#   kubernetes.io/hostname
#   failure-domain.beta.kubernetes.io/zone
#   failure-domain.beta.kubernetes.io/region
#   topology.kubernetes.io/zone
#   topology.kubernetes.io/region
#   beta.kubernetes.io/instance-type
#   node.kubernetes.io/instance-type
#   kubernetes.io/os
#   kubernetes.io/arch

# Documentation:
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector

# Example
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  nodeSelector:
    kubernetes.io/os: linux
  containers:
  - name: busybox
    image: busybox:1.34
    command:
    - sleep
    - "300"
    
# In that case, Pod will be assigned to the node with kubernetes.io/os=linux label. 
# There’s a list of predefined labels, but you can also use your custom labels on hosts.

# Task:
# We’ve already created exercise deployment for you.

# Let’s split nodes of our cluster to zones:

# master node: topology.kubernetes.io/zone=eu-east-1a label
# node01 node: topology.kubernetes.io/zone=eu-east-1b label
# node02 node: topology.kubernetes.io/zone=eu-east-1c label

# Update exercise deployment:
# All replicas should run on the node(s) with topology.kubernetes.io/zone=eu-east-1c label

apiVersion: apps/v1
kind: Deployment
metadata:
  name: exercise-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: exercise-app
  template:
    metadata:
      labels:
        app: exercise-app
    spec:
      nodeSelector:
        topology.kubernetes.io/zone: eu-east-1c
      containers:
      - name: exercise-container
        image: exercise-image
        
# Solution:

        
# # For example, to label the master node with topology.kubernetes.io/zone=eu-east-1a:
# $ kubectl label nodes master topology.kubernetes.io/zone=eu-east-1a
# # For node01 node with topology.kubernetes.io/zone=eu-east-1b:
# $ kubectl label nodes node01 topology.kubernetes.io/zone=eu-east-1b
# # For node02 node with topology.kubernetes.io/zone=eu-east-1c:
# $ kubectl label nodes node02 topology.kubernetes.io/zone=eu-east-1c

# Once the nodes are labeled, you can then update your deployment to schedule pods on specific nodes based on the topology.kubernetes.io/zone label by using nodeSelector.
# $ kubectl patch deployment exercise -p '{"spec": {"template": {"spec": {"nodeSelector": {"topology.kubernetes.io/zone": "eu-east-1c"}}}}}'

# Verify:
# Checking Node Topology Lables:

# $ kubectl get nodes --label-columns=topology.kubernetes.io/region,topology.kubernetes.io/zone
# NAME     STATUS   ROLES                         AGE   VERSION        REGION      ZONE
# node01   Ready    worker                        15m   v1.21.5+k3s2   eu-east-1   eu-east-1b
# node02   Ready    worker                        15m   v1.21.5+k3s2   eu-east-1   eu-east-1c
# master   Ready    control-plane,master,worker   15m   v1.21.5+k3s2   eu-east-1   eu-east-1a

# Checking all pods scheduled to the proper zone:

# $ kubectl describe pod exercise-... | grep 'Node'
# Node:         node02/172.31.0.4
# Node-Selectors:  topology.kubernetes.io/zone=eu-east-1c
# ...

# $ kubectl get pods -o wide
# NAME                        READY   STATUS    RESTARTS   AGE   IP          NODE     NOMINATED NODE   READINESS GATES
# exercise-6b89dc68f9-qxg9j   1/1     Running   0          18s   10.42.1.6   node02   <none>           <none>
# exercise-6b89dc68f9-dldbj   1/1     Running   0          19s   10.42.1.5   node02   <none>           <none>
# exercise-6b89dc68f9-hm7tm   1/1     Running   0          18s   10.42.1.7   node02   <none>           <none>

########################################################################################################################################

# 3. Node Affinity
# Brief overview:
# https://youtu.be/6ZHjqpn9dck
# The nodeSelector provides a very simple way to constrain pods to nodes with particular labels.

# The affinity/anti-affinity feature greatly expands the types of constraints you can express. The key enhancements are
# - The affinity/anti-affinity language offers more matching rules besides exact matches created with a logical AND operation
# - You can indicate that the rule is “soft” / “preference” rather than a hard requirement, so if the scheduler can’t satisfy it, the pod will still be scheduled
# - You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located

# The Affinity feature consists of two types of affinity:
# - “node affinity” - like the existing nodeSelector (but with the first two benefits listed above)
# - “inter-pod affinity/anti-affinity” - constrains against pod labels rather than node labels, as described in the 3rd item listed above, in addition to having the 1st and 2nd properties listed above

# Node affinity
# Node affinity is conceptually similar to nodeSelector – it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.

# There are currently two types of node affinity:
# - requiredDuringSchedulingIgnoredDuringExecution (hard)
# - preferredDuringSchedulingIgnoredDuringExecution (soft)

# You can think of them as hard and soft respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (just like nodeSelector but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee.

# The IgnoredDuringExecution part of the names means that, similar to how nodeSelector works, if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod will still continue to run on the node

# Node affinity is specified as field affinity.nodeAffinity in the PodSpec
# Example:

apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: topology.kubernetes.io/region
            operator: In
            values:
            - us-west-1
            - us-east-1
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: node.kubernetes.io/instance-type
            operator: In
            values:
            - m5.4xlarge
  containers:
  - name: with-node-affinity
    image: k8s.gcr.io/pause:3.3
    
# This node affinity rule says the pod can only be placed on a node with a label whose key is topology.kubernetes.io/region and whose value is either us-west-1 or us-east-1. In addition, among nodes that meet that criteria, nodes with a label whose key is node.kubernetes.io/instance-type and whose value is m5.4xlarge should be preferred.

# The weight field in preferredDuringSchedulingIgnoredDuringExecution is in the range 1-100. For each node that meets all of the scheduling requirements (resource request, RequiredDuringScheduling affinity expressions, etc.), the scheduler will compute a sum by iterating through the elements of this field and adding “weight” to the sum if the node matches the corresponding MatchExpressions. This score is then combined with the scores of other priority functions for the node. 
# The node(s) with the highest total score are the most preferred.

# Task:
# As far as you know, all the hosts of our cluster are split by zones:

# $ kubectl get nodes -o custom-columns=NAME:.metadata.name,ZONE:'.metadata.labels.topology\.kubernetes\.io/zone'
# NAME     ZONE
# node02   eu-east-1c
# master   eu-east-1a
# node01   eu-east-1b
# We have also added performance label

# Rework exercise deployment to spread pods across the hosts from eu-east-1b and eu-east-1c zones with the preferred best performance (good is also acceptable)

#  Don’t change Nodes’ labels

apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "2"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"1"},"creationTimestamp":"2023-01-23T08:52:53Z","generation":1,"labels":{"app":"exercise"},"name":"exercise","namespace":"task-1","resourceVersion":"533","uid":"ca0287b4-fb51-4e7d-9fb1-ec4eb52de704"},"spec":{"progressDeadlineSeconds":600,"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"exercise"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"exercise"}},"spec":{"containers":[{"image":"k8s.gcr.io/pause:3.3","imagePullPolicy":"IfNotPresent","name":"exercise","resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","nodeSelector":{"topology.kubernetes.io/zone":"eu-east-1c"},"restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30}}},"status":{"availableReplicas":1,"conditions":[{"lastTransitionTime":"2023-01-23T08:52:54Z","lastUpdateTime":"2023-01-23T08:52:54Z","message":"Deployment has minimum availability.","reason":"MinimumReplicasAvailable","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-23T08:52:54Z","lastUpdateTime":"2023-01-23T08:52:54Z","message":"ReplicaSet \"exercise-76cb7f6c4f\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"}],"observedGeneration":1,"readyReplicas":1,"replicas":1,"updatedReplicas":1}}
  creationTimestamp: "2023-01-23T09:08:32Z"
  generation: 2
  labels:
    app: exercise
  name: exercise
  namespace: task-3
  resourceVersion: "2001"
  uid: a2551beb-fd24-400e-b554-0a0da51b55a2
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: exercise
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: exercise
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: performance
                operator: In
                values:
                - best
                - good
            weight: 1
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/zone
                operator: In
                values:
                - eu-east-1b
                - eu-east-1c
      containers:
      - image: k8s.gcr.io/pause:3.3
        imagePullPolicy: IfNotPresent
        name: exercise
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      nodeSelector:
        topology.kubernetes.io/zone: eu-east-1c
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30


# Verify:
# Use these commands:
# $ kubectl get nodes --label-columns=topology.kubernetes.io/zone,performance
# $ kubectl get pods -o wide

# Documentation:
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity


########################################################################################################################################


# 4. Inter-Pod Affinity

# https://youtu.be/0OXX14DUMcI

# Problem
# We need to run application pod(s) where other application pods are running already.
# For example, a web application has in-memory cache such as redis. We want the web-servers to be co-located with the cache as much as possible.

# Theory
# Inter-pod affinity and anti-affinity allow you to constrain which nodes your pod is eligible to be scheduled based on labels on pods that are already running on the node rather than based on labels on nodes.

# As with node affinity, there are currently two types of pod affinity and anti-affinity:
# - requiredDuringSchedulingIgnoredDuringExecution
# - preferredDuringSchedulingIgnoredDuringExecution

# An example of requiredDuringSchedulingIgnoredDuringExecution affinity would be “co-locate the pods of service A and service B in the same zone, since they communicate a lot with each other” and an example preferredDuringSchedulingIgnoredDuringExecution anti-affinity would be “spread the pods from this service across zones” (a hard requirement wouldn’t make sense, since you probably have more pods than zones).

# Inter-pod affinity is specified as field affinity.podAffinity in the PodSpec. And inter-pod anti-affinity is specified as field affinity.podAntiAffinity in the PodSpec.

# Example
# The below yaml snippet of the webserver deployment has podAffinity configured. This informs the scheduler that all its replicas are to be co-located with pods that have selector label app=store. This will also ensure that each web-server replica does not co-locate on a single node.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-app
  replicas: 3
  template:
    metadata:
      labels:
        app: web-app
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx:alpine
        
# What is a topologyKey?
# It is the key of node labels. If two Nodes are labeled with this key and have identical values for that label, the scheduler treats both Nodes as being in the same topology. The scheduler tries to place a balanced number of Pods into each topology domain.

# Just a reminder: Built-in node labels
# In addition to the labels you attach, nodes come pre-populated with a standard set of labels. These labels are:
#   kubernetes.io/os
#   kubernetes.io/arch
#   kubernetes.io/hostname
#   topology.kubernetes.io/zone
#   topology.kubernetes.io/region
#   node.kubernetes.io/instance-type

# Task:
# We have cache and app deployments running in the cluster. Need to make sure app’s pods are always collocated with cache’s pods on the same nodes.
# Please update app deployment specification with Inter-pod affinity settings.
# Do not change cache deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "3"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{"deployment.kubernetes.io/revision":"3"},"creationTimestamp":"2023-01-23T09:56:45Z","generation":3,"labels":{"app":"app"},"name":"app","namespace":"task-4","resourceVersion":"3168","uid":"bede3260-d722-4b80-8ae6-6469edacd844"},"spec":{"progressDeadlineSeconds":600,"replicas":1,"revisionHistoryLimit":10,"selector":{"matchLabels":{"app":"app"}},"strategy":{"rollingUpdate":{"maxSurge":"25%","maxUnavailable":"25%"},"type":"RollingUpdate"},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"app"}},"spec":{"affinity":{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["cache"]}]},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"image":"k8s.gcr.io/pause:3.3","imagePullPolicy":"IfNotPresent","name":"app-server","resources":{},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File"}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","schedulerName":"default-scheduler","securityContext":{},"terminationGracePeriodSeconds":30}}},"status":{"availableReplicas":1,"conditions":[{"lastTransitionTime":"2023-01-23T09:56:46Z","lastUpdateTime":"2023-01-23T09:56:46Z","message":"Deployment has minimum availability.","reason":"MinimumReplicasAvailable","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-23T09:56:45Z","lastUpdateTime":"2023-01-23T10:10:27Z","message":"ReplicaSet \"app-7c8c5f4db5\" has successfully progressed.","reason":"NewReplicaSetAvailable","status":"True","type":"Progressing"}],"observedGeneration":3,"readyReplicas":1,"replicas":1,"updatedReplicas":1}}
  creationTimestamp: "2023-01-23T10:11:20Z"
  generation: 3
  labels:
    app: app
  name: app
  namespace: task-4
  resourceVersion: "3824"
  uid: d2ad2414-27b5-4222-8dd0-aa8c4cc74927
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: app
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: app
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: cache
                operator: In
                values:
                - "yes"
            topologyKey: kubernetes.io/hostname
      containers:
      - image: k8s.gcr.io/pause:3.3
        imagePullPolicy: IfNotPresent
        name: app-server
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30

# Verify:
# ### Before
# $ kubectl get pods -o wide
# NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
# app-6cc9885d55-mmccq     1/1     Running   0          17s   10.42.1.13   master   <none>           <none>
# cache-76786b5cbb-kl272   1/1     Running   0          17s   10.42.2.12   node02   <none>           <none>

# ### After
# $ kubectl get pods -o wide
# NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
# cache-76786b5cbb-kl272   1/1     Running   0          35s   10.42.2.12   node02   <none>           <none>
# app-5d59f4bb45-wnpqn     1/1     Running   0          8s    10.42.2.13   node02   <none>           <none>

# Documentation:
# https://kubernetes.io/docs/reference/labels-annotations-taints/
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#always-co-located-in-the-same-node

##################################################################################################################################

# 5. Pod Anti-Affinity

# https://youtu.be/0OXX14DUMcI
# Problem
# We need to make sure all replicas (pods) of our application are running on different nodes (for high availability if the host goes down accidentally, or to spread the load across nodes, etc)

# Theory
# Inter-pod affinity and anti-affinity allow you to constrain which nodes your pod is eligible to be scheduled based on labels on pods that are already running on the node rather than based on labels on nodes.

# As with node affinity, there are currently two types of pod affinity and anti-affinity:
#   requiredDuringSchedulingIgnoredDuringExecution
#   preferredDuringSchedulingIgnoredDuringExecution

# An example of requiredDuringSchedulingIgnoredDuringExecution affinity would be “co-locate the pods of service A and service B in the same zone, since they communicate a lot with each other” and an example preferredDuringSchedulingIgnoredDuringExecution anti-affinity would be “spread the pods from this service across zones” (a hard requirement wouldn’t make sense, since you probably have more pods than zones).

# Inter-pod affinity is specified as the field affinity.podAffinity in the PodSpec. And inter-pod anti-affinity is specified as the field affinity.podAntiAffinity in the PodSpec.

# Example
# Hard:

apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - exercise
        topologyKey: "kubernetes.io/hostname"
  containers:
  - name: with-pod-anti-affinity
    image: k8s.gcr.io/pause:3.3
    
# Soft:
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: failure-domain.beta.kubernetes.io/zone
  containers:
  - name: with-pod-anti-affinity
    image: k8s.gcr.io/pause:3.3
    
# The pod anti-affinity rule says that the pod cannot be scheduled onto a node if that node is in the same zone as a pod with label having key “security” and value “S2”.

# Task:
# There’s rival deployment with 2 pods

# Add to the deployment above pod anti-affinity rules with the following requirements:

# Both deployment pods should always avoid app deployment and run in different (to app) zones
# Choose the only “hard” or “soft” requirement (type)
# Choose proper topologyKey
#  Do not change app deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "11"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"rival"},"name":"rival","namespace":"task-5"},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"rival"}},"template":{"metadata":{"labels":{"app":"rival"}},"spec":{"containers":[{"image":"k8s.gcr.io/pause:3.3","name":"main"}]}}}}
  creationTimestamp: "2023-01-23T10:08:07Z"
  generation: 11
  labels:
    app: rival
  name: rival
  namespace: task-5
  resourceVersion: "5560"
  uid: 960c9494-ed45-4666-a6dc-6a4c55b44c3b
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: rival
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: rival
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - app-server
            topologyKey: topology.kubernetes.io/zone
      containers:
      - image: k8s.gcr.io/pause:3.3
        imagePullPolicy: IfNotPresent
        name: main
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30

# Verify:
# Before (uncontrolled):

# $ kubectl get pods -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase
# NAME                     NODE     STATUS
# app-766d6bf6b5-qq498     master   Running
# rival-56c7d6bb5f-dhpg2   master   Running
# rival-56c7d6bb5f-qcxh7   node02   Running
# After (now we’re sure that all pods are running on different nodes):

# $ kubectl get pods -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase
# NAME                     NODE     STATUS
# app-766d6bf6b5-qq498     master   Running
# rival-56c7d6bb5f-dhpg2   node01   Running
# rival-56c7d6bb5f-qcxh7   node02   Running
# Documentation:
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
# https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#never-co-located-in-the-same-node


####################################################################################################################################

# 6. Taints and Tolerations
# https://youtu.be/mo2UrkjA7FE

# Theory
# Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement).
# Taints are the opposite - they allow a node to repel a set of pods.
# Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.
# Taints and Tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.
# Tolerations are often combined with node affinity or node selector parameters to guarantee that only the matched nodes are used for pod scheduling.

# Brief Summary
#   Node Affinity is a property of Pod - where to run
#   Taint is a property of Node - what is allowed to run (schedule/execute) on it
#   Tolerations is a property of Pod - how to satisfy Node’s taint

# Taints Configuration

# On Node Side:
# You can add a taint to a node using kubectl taint. For example,

# $ kubectl taint nodes node1 key1=value1:NoSchedule

# It sets a taint on node node1. This taint has key1 key, value1 value, and taint effect of NoSchedule. This means that no pod will be able to schedule onto node1 unless it matches toleration.

# To remove the taint from node, you can run:

# $ kubectl taint nodes node1 key1:NoSchedule-
# To get all tains on all nodes use the command:

# $ kubectl get nodes -o json | jq '.items[].spec.taints'
# $ kubectl get node << node name >> -o json | jq '.spec.taints'

# Task:
# Create taints according to the following rules:
# on "master" node: taint with a key of robot, value of starwars and effect of NoSchedule
# on "node01" node: taint with a key of robot, value of terminator and effect of NoSchedule

# SOLUTION 

# To create a taint with a key of robot, value of starwars, and effect of NoSchedule on the "master" node, you would run the following command:
# $ kubectl taint nodes master robot=starwars:NoSchedule

# Similarly, to create a taint with a key of robot, value of terminator, and effect of NoSchedule on the "node01" node, you would run the following command:
# $ kubectl taint nodes node01 robot=terminator:NoSchedule
# This will create a taint on the specified node, and any pods that do not tolerate the taint will not be scheduled on that node.

# Documentation:
# https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/

##############################################################################################################################

# 7. Taints and Tolerations
# https://youtu.be/mo2UrkjA7FE

# Toleration Configuration
# Taints and Tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. 
# One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.

# Tolerations are often combined with node affinity or node selector parameters to guarantee that only the matched nodes are used for pod scheduling.

# On Node Side:
# You can add a taint to a node using kubectl taint. For example:
# $ kubectl taint nodes node1 key1=value1:NoSchedule

# On Pod Side:
# You specify a toleration for a pod in the PodSpec. Both of the following tolerations “match” the taint created by the kubectl taint line above, and thus a pod with either toleration would be able to schedule onto node1:

tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
Full Pod Definition Example:

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx:alpine
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"
    
Taint and Toleration Components:

# key	
# The key is any string, up to 253 characters. The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores

# value	
# The value is any string, up to 63 characters. The value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores

# effect	
# The effect is one of the following:
# - NoSchedule - New pods that do not match the taint are not
# scheduled onto that node.
# - PreferNoSchedule - is a “preference” or “soft” version of NoSchedule
# - NoExecute - New pods that do not match the taint cannot be scheduled onto that node

# operator	
# One of the following:
# - Equal - The key/value/effect parameters must match. This is the default.
# - Exists - The key/effect parameters must match. You must leave a blank value parameter, which matches any
# For more details, please look at the documentation section.

# Task:
# Please investigate what taint is set on node01 and add corresponding toleration to the current deployment exercise (/tasks/7/app.yml). Deploy it.

# SOLUTION
# $ kubectl describe node node01 | grep Taint

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: exercise
  name: exercise
spec:
  replicas: 1
  selector:
    matchLabels:
      app: exercise
  template:
    metadata:
      labels:
        app: exercise
    spec:
      tolerations:
      - key: "env"
        operator: "Equal"
        value: "production"
        effect: "NoSchedule"
      containers:
      - image: k8s.gcr.io/pause:3.3
        name: main

# Documentation:
# https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/


##########################################################################################

8. Taints and Tolerations Troubleshooting

# Task:
# Please troubleshoot and fix stranger “production” deployment

# $ kubectl get deploy
# NAME       READY   UP-TO-DATE   AVAILABLE   AGE
# stranger   0/2     2            0           38s

# $ kubectl get po
# NAME                        READY   STATUS    RESTARTS   AGE
# stranger-759f9cc947-h2w4j   0/1     Pending   0          38s
# stranger-759f9cc947-d9zrz   0/1     Pending   0          38s
# Hint:
#  Start from describing its pod(s) - “Events” section. Do not change nodes settings. Your goal is to fix the pods

# SOLUTION:
# Check if TAINTS on NODES matches with TOLERATIONS on PODS/DEPLOYMENT:
# on podes:
#       tolerations:
#       - effect: NoSchedule
#         key: env
#         value: production
#       - effect: NoSchedule
#         key: protect
#         value: critical
# on node:
# Taints:             env=production:NoSchedule
#                     protect=critical:NoSchedule

# Documentation:
# https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/

###########################################################

# Taints and Tolerations Eviction Control

# Normally, if a taint with effect NoExecute is added to a node, then any pods that do not tolerate the taint will be evicted immediately, and pods that do tolerate the taint will never be evicted. However, a toleration with NoExecute effect can specify an optional tolerationSeconds field that dictates how long the pod will stay bound to the node after the taint is added. For example,

tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600
# means that if this pod is running and a matching taint is added to the node, then the pod will stay bound to the node for 3600 seconds, and then be evicted. If the taint is removed before that time, the pod will not be evicted.

# Task:
# Follow these steps:

# 1. Create r2d2 deployment as required below:
# image: k8s.gcr.io/pause:3.3
# replicas: 2
# should run on node01 (tolerate node01 taints)

apiVersion: apps/v1
kind: Deployment
metadata:
  name: r2d2
spec:
  replicas: 2
  selector:
    matchLabels:
      app: r2d2
  template:
    metadata:
      labels:
        app: r2d2
    spec:
      containers:
      - name: r2d2
        image: k8s.gcr.io/pause:3.3
      tolerations:
      - key: env
        value: starwars
        effect: NoSchedule

# 2. Check that its pods are running
# Expected Result:

# $ kubectl get pods
# NAME                   READY   STATUS    RESTARTS   AGE
# r2d2-9b95dcf57-27pc5   1/1     Running   0          5s
# r2d2-9b95dcf57-5xh4r   1/1     Running   0          5s

# 3. Add taint robots=any with effect NoExecute to node01

# $ kubectl taint nodes node01 robots=any:NoExecute

# 4. Check what happens
# Expected Result:

# Pods Evicting:
# $ kubectl get pods
# NAME                   READY   STATUS        RESTARTS   AGE
# r2d2-9b95dcf57-27pc5   0/1     Terminating   0          88s
# r2d2-9b95dcf57-5xh4r   1/1     Terminating   0          88s
# r2d2-9b95dcf57-97gc8   0/1     Pending       0          3s
# r2d2-9b95dcf57-tqws9   0/1     Pending       0          3s
# Pods Evicted, but there’s no node where they can be scheduled to:

# $ kubectl get pods
# NAME                   READY   STATUS    RESTARTS   AGE
# r2d2-9b95dcf57-97gc8   0/1     Pending   0          59s
# r2d2-9b95dcf57-tqws9   0/1     Pending   0          59s

# Documentation:
# https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/


###################

# 11. Node Maintenance

# You can use kubectl drain to safely evict all of your pods (simply, making it disabled for scheduling) from a node before you perform maintenance on the node (e.g. kernel upgrade, hardware maintenance, etc.).

# kubectl drain {%node name%}
# kubectl drain {%node name%} --ignore-daemonsets
# This keeps new pods from landing on the node while you are trying to get them off.

# For pods with a replica set, the pod will be replaced by a new pod which will be scheduled to a new node. Additionally, if the pod is part of a service, then clients will automatically be redirected to the new pod.

# For pods with no replica set, you need to bring up a new copy of the pod, and assuming it is not part of a service, redirect clients to it. Once it returns (without giving an error), you can power down the node (or equivalently, if on a cloud platform, delete the virtual machine backing the node).

# Task:
# Turn off scheduling on node02 (it will trigger rescheduling of all pods running on this node)

# SOLUTION
# $ kubectl cordon node02

# Sample result:
# $ kubectl get nodes
# NAME     STATUS                     ROLES                         AGE   VERSION
# master   Ready                      control-plane,master,worker   37m   v1.20.0+k3s2
# node01   Ready                      worker                        37m   v1.20.0+k3s2
# node02   Ready,SchedulingDisabled   worker                        37m   v1.20.0+k3s2

# Documentation:
# https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/
